= AMQ Streams (Kafka) demo

The goal of this demo is to show some of the key capabilities of Red Hat AMQ Streams.

One of the key feature of *Red HatÂ® AMQ Streams* is bringing the power of *Apache Kafka* in *OpenShift Container Platform*, leveraging the Strimzi project.footnote:[https://strimzi.io/]

== Create Kafka broker and topic

Create a Broker with metrics enabled:

- consumer lag
- consumer offsets

[source,ruby]
----
oc new-project my-kafka
oc apply -f k8s/01-kafka-metrics.yaml
oc apply -f k8s/02-kafkatopic.yaml
----

== Enabling monitoring

NOTE: This demo relies on the _cluster-admin_ role.

Create cluster-monitoring-config ConfigMap object:

[source,shell]
----
oc apply -f k8s/03-cluster-monitor.yaml
----

Check that the prometheus-operator, prometheus-user-workload and thanos-ruler-user-workload pods are running in the openshift-user-workload-monitoring project. It might take a short while for the pods to start:

[source,shell]
----
oc -n openshift-user-workload-monitoring get pod
----

Enable:
- monitoring for kafka resources
- prometheus rules


[source,shell]
----
oc apply -f k8s/04-pod-monitor.yaml -n my-kafka
oc apply -f k8s/05-prometheus-rules.yaml -n my-kafka
----

Create a service account for Grafana:

[source,shell]
----
oc apply -f k8s/06-graphana-auth.yaml -n my-kafka
----

Create Prometheus datasource for Grafana and deploy Grafana

[source,shell]
----
k8s/07-create-datasource.sh
oc apply -f k8s/08-grafana.yaml
oc create route edge --service=grafana --namespace=my-kafka
----

Login with the default credentials (`admin/admin`) and then change the password.

Load the dashboard definitions from `grafana-dashboards` folder:

- `strimzi-kafka.json`
- `strimzi-kafka-exporter.json`

== Install the consumer and producer applications

[source,shell]
----
mvn -f kafka-consumer/pom.xml package -Dquarkus.kubernetes.deploy=true -DskipTests
mvn -f kafka-producer/pom.xml package -Dquarkus.kubernetes.deploy=true -DskipTests
----

Further information about the applications:

* xref:kafka-consumer/README.md[Consumer App Readme]

* xref:kafka-producer/README.md[Producer App Readme]

== Demo routines

=== AMQ Streams High Availability

. Show consumer logs
+
[source,shell]
----
oc logs --tail=20 -f --selector="app.kubernetes.io/name=kafka-consumer"
----

. Show producer logs
+
[source,shell]
----
oc logs --tail=20 -f --selector="app.kubernetes.io/name=kafka-producer"
----

. Show the topic distribution
+
[source,shell]
----
oc exec -it my-cluster-kafka-0 -- bin/kafka-topics.sh \
                                --bootstrap-server my-cluster-kafka-bootstrap:9092 \
                                --describe --topic event
----

. Test the Kafka's resiliance and consistency by brutely shutting down one of brokers' pod.
+
TIP: Use the following command: `oc delete --force pod <pod-name>`

. Show again the topic distribution on the cluster members

. Show the dashboard

=== AMQ Streams Rebalancing

. Add the cruise control
+
[source,shell]
----
oc patch kafka my-cluster --patch '{"spec":{"cruiseControl": {}}}' --type=merge
----

. Point out the partition leaders
+
[source,shell]
----
oc exec -it my-cluster-kafka-0 -- bin/kafka-topics.sh \
            --bootstrap-server my-cluster-kafka-bootstrap:9092 \
            --describe --topic event
----

. Change the producer to create messages on a set of partitions which have the same leader
+
[source,shell]
----
oc edit configmap kafka-producer-config
----
+
update the variables in order to produce only on selected partitions, e.g.:
+
[source,shell]
----
  PRODUCER_PARTED: "true"
  PRODUCER_PARTITIONS: 0,3,6
----
+
bump the kafka producer application:
+
[source,shell]
----
oc scale deployment/kafka-producer --replicas=0
oc scale deployment/kafka-producer --replicas=1
----

. Check that partitions grows at different paces, running the following command:
+
[source,shell]
----
oc exec -it my-cluster-kafka-0 -- bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list localhost:9092 --topic event
----

. Trigger the rebalance analysis:
+
[source,shell]
----
oc apply -f k8s/09-kafka-rebalance-full.yaml
----

. Review the optimization proposal:
+
[source,shell]
----
oc describe kafkarebalance my-rebalance
----

. Approve the proposal
+
[source,shell]
----
oc annotate kafkarebalances.kafka.strimzi.io my-rebalance strimzi.io/rebalance=approve
----
+
later on you can refresh the analysis:
+
[source,shell]
----
oc annotate kafkarebalances.kafka.strimzi.io my-rebalance strimzi.io/rebalance=refresh
----

. Rebalancing takes some time, run again the following command and wait for `Status: True`
+
[source,shell]
----
oc describe kafkarebalance my-rebalance
----

. Run again the decribe topic command, you should spot the overloaded partitions moved on different leaders:
+
[source,shell]
----
oc exec -it my-cluster-kafka-0 -- bin/kafka-topics.sh \
            --bootstrap-server my-cluster-kafka-bootstrap:9092 \
            --describe --topic event
----

=== Persist Consumed Messages

In this section, the consumer is enhanced to store the messages in a Postgres DB.
The goal is to show the client scalability and resiliance

. Stop the consumer and producer and restore the normal producer behavior
+
[source,shell]
----
oc scale deployment kafka-consumer --replicas=0
oc scale deployment kafka-producer --replicas=0
oc edit configmap kafka-producer-config
----
+
update the evironment variable to its original value
+
[source,shell]
----
  PRODUCER_PARTED: "false"
----

. Deploy the database (basic ephemaral deployment)
+
[source,shell]
----
oc create configmap event-db-init-data --from-file=./kafka-producer/src/main/resources/import.sql
oc apply -f k8s/20-postgres.yaml
----

. Edit the consumer configuration to enable persistence:
+
[source,shell]
----
oc edit configmap/kafka-consumer-config
----
+
Change this environment variable: `TRACKING_DB: "true"`

. Reset the producer and the consumer
+
[source,shell]
----
oc scale deployment kafka-consumer --replicas=1
oc scale deployment kafka-producer --replicas=1
----

. Deploy a simple Python application to poll the DB and detect duplicate or missing messages
+
[source,shell]
----
oc new-build --strategy docker --binary --name=db-watcher
oc start-build db-watcher --from-dir python-db-watcher/ --follow
oc new-app -l app.kubernetes.io/part-of=event-application -e POSTGRES_SVC=event-db db-watcher
----

. Open the db-watcher logs
+
[source,shell]
----
oc logs --tail 10 -f --selector="deployment=db-watcher"
----
+
NOTE: At this point, there should be no missing or duplicate messages, so the log should be empty.

. Scale up the consumer
+
[source,shell]
----
oc scale deployment kafka-consumer --replicas=2
----
+
NOTE: When the new consumer pod become active, the other consumer gives up half of his partitions to the new one. For a while you should spot some missing messages in `db-watcher` log. However, it's a transient condition.

. Test the consumer's resiliance and consistency by brutely shutting down one of the two pods.
+
TIP: Use the following command: `oc delete --force pod <pod-name>`

== Clean up

In order to start the demo from scratch, with minimal effort: delete only the kafka broker and the topics:

[source,shell]
----
oc delete kafkatopics --selector="strimzi.io/cluster=my-cluster"
oc delete kafka my-cluster
----

Drop the PVC:

[source,shell]
----
oc delete pvc --selector="strimzi.io/cluster=my-cluster"
----

Then, you can apply again the first two yaml files.

=== Database clean up

[source,shell]
----
oc rsh event-db-<id>
$ psql -U quarkus quarkus
quarkus=> DELETE FROM event;
quarkus=> ALTER SEQUENCE event_seq RESTART WITH 1;
----
