= AMQ Streams (Kafka) demo

The goal of this demo is to show some of the key capabilities of Red Hat AMQ Streams.

One of the key feature of *Red HatÂ® AMQ Streams* is bringing the power of *Apache Kafka* in *OpenShift Container Platform*, leveraging the Strimzi project.footnote:[https://strimzi.io/]

== Create Kafka broker and topic

Create a Broker with metrics enabled:

- consumer lag
- consumer offsets

[source,ruby]
----
oc new-project my-kafka
oc apply -f k8s/01-kafka-metrics.yaml
oc apply -f k8s/02-kafkatopic.yaml
----

== Enabling monitoring

NOTE: This demo relies on the _cluster-admin_ role.

Create cluster-monitoring-config ConfigMap object:

[source,shell]
----
oc apply -f k8s/03-cluster-monitor.yaml
----

Check that the prometheus-operator, prometheus-user-workload and thanos-ruler-user-workload pods are running in the openshift-user-workload-monitoring project. It might take a short while for the pods to start:

[source,shell]
----
oc -n openshift-user-workload-monitoring get pod
----

Enable:
- monitoring for kafka resources
- prometheus rules


[source,shell]
----
oc apply -f k8s/04-pod-monitor.yaml -n my-kafka
oc apply -f k8s/05-prometheus-rules.yaml -n my-kafka
----

Create a service account for Grafana:

[source,shell]
----
oc apply -f k8s/06-graphana-auth.yaml -n my-kafka
----

Create Prometheus datasource for Grafana and deploy Grafana

[source,shell]
----
k8s/07-create-datasource.sh
oc apply -f k8s/08-grafana.yaml
oc create route edge --service=grafana --namespace=my-kafka
----

Login with the default credentials (`admin/admin`) and then change the password.

Load the dashboard definitions from `grafana-dashboards` folder:

- `strimzi-kafka.json`
- `strimzi-kafka-exporter.json`

== Install the consumer and producer applications

Follow the instuctions:


* file:///kafka-consumer/README.md[Consumer App Readme]

* file:///kafka-producer/README.md[Producer App Readme]

== Demo routines

. Show consumer logs
+
[source,shell]
----
oc logs --tail=20 -f --selector="app.kubernetes.io/name=kafka-consumer"
----

. Show producer logs
+
[source,shell]
----
oc logs --tail=20 -f --selector="app.kubernetes.io/name=kafka-producer"
----

. Show the topic distribution
+
[source,shell]
----
oc exec -it my-cluster-kafka-0 -- bin/kafka-topics.sh \
                                --bootstrap-server my-cluster-kafka-bootstrap:9092 \
                                --describe --topic event
----

. Kill one of the broker

. Show again the topic distribution on the cluster members

. Show the dashboard


== Clean up

Delete only the kafka broker and the topics:

[source,shell]
----
oc delete kafkatopics --selector="strimzi.io/cluster=my-cluster"
oc delete kafka my-cluster
----

Drop the PVC:

[source,shell]
----
oc delete pvc --selector="strimzi.io/cluster=my-cluster"
----
