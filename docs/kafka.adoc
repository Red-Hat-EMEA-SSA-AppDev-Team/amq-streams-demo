= Core Kafka installation and operations

== Create Kafka broker and topic

Add **AMQ Streams** operator to your OpenShift environment.

Create a Broker with metrics enabled:

- consumer lag
- consumer offsets

[source,ruby]
----
oc new-project my-kafka
oc apply -f k8s/basics/01-metrics-configmap.yaml
oc apply -f k8s/basics/02-kafka.yaml
oc apply -f k8s/basics/03-kafkatopic.yaml
----

== Enabling user workload monitoring

NOTE: This section relies on the _cluster-admin_ role.

The following command add many resources in OpenShift through _kustomizer_:

[source,shell]
----
oc apply -k k8s/monitor
----

Specifically it does the following:

* adds a _ConfigMap_ named `cluster-monitoring-config` in the `openshift-monitoring` namespace: it triggers the deployment of Prometheus and Thanos. Grafana uses the Thanos Querier which works as Prometheus proxy to scrape the metrics.
* creates a _ClusterRoleBinding_ which grants permissions to `grafana-serviceaccount`
* pod monitoring rules for kafka resources
* prometheus rules
* deploy Grafana dashboard

[TIP] 
====
If you chose a different _namespace_ to install the demo artifact, you can update accordingly the following files:

* `k8s/monitor/clusterrolebinding.patch.yaml`
* `k8s/monitor/podmonitor.patch.yaml`
====

The following script:

* creates a _service account_ for Grafana with an _access token_ (it lasts 30 days)
* creates a datasource configuration for Grafana to scrape the metrics. It points to Thanos Querier which acts as a Prometheus proxy and uses the previous created _access token_ to gain access. 

[source,shell]
----
k8s/monitor/01-create-datasource.sh
----

Optionally, check that the `prometheus-operator`, `prometheus-user-workload` and `thanos-ruler-user-workload` pods are running in the `openshift-user-workload-monitoring` project.

[source,shell]
----
oc -n openshift-user-workload-monitoring get pod
----

Expose grafana:

[source,shell]
----
oc create route edge --service=grafana
----

Login with the default credentials (`admin/admin`) and then change the password.

Load the dashboard definitions from `grafana-dashboards` folder:

- `strimzi-kafka.json`
- `strimzi-kafka-exporter.json`

== Install the consumer and producer applications

[source,shell]
----
mvn -f kafka-consumer package -Dquarkus.kubernetes.deploy=true -DskipTests
mvn -f kafka-producer package -Dquarkus.kubernetes.deploy=true -DskipTests
----

Further information about the applications:

* xref:../kafka-consumer/README.md[Consumer App Readme]

* xref:../kafka-producer/README.md[Producer App Readme]

== Demo routines

=== AMQ Streams High Availability

. Show consumer logs
+
[source,shell]
----
oc logs -f deployments/kafka-consumer
----

. Show producer logs
+
[source,shell]
----
oc logs -f deployments/kafka-producer
----

. Show the partitions distribution
+
[source,shell]
----
oc exec -it my-cluster-controller-3 -- \
    bin/kafka-topics.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 \
    --describe --topic event
----

. Test the Kafka's resilience and consistency by forcefully shutting down one of brokers' pod.
+
TIP: Use the following command: `oc delete --force pod <pod-name>`
+
IMPORTANT: Watching at the consumer log you should notice that it temporarily stops processing some messages (missing messages). This is expected! In fact, Kafka promotes consistency over availability, so until a new partition leader is elected you cannot write or consume messages on that partition. Eventually, the new leader will become available and the missing messages will be caught up.

. Show again the topic distribution on the cluster members

. Show the dashboard

== Appendix

=== Refresh Grafana token

Prometheus access token lasts 30 days.

To refresh it:

[source,shell]
----
oc delete configmap grafana-config
k8s/monitor/01-create-datasource.sh
oc delete pod --selector name=grafana
----

=== Full Grafana clean up

Delete Grafana deployment:

[source,shell]
----
oc delete all -l app=kafka-monitor
oc delete podmonitor -l app=kafka-monitor
oc delete sa -l app=kafka-monitor
oc delete pvc -l app=kafka-monitor
oc delete configmap grafana-config
----

=== Entities segregation

Segregating cluster configuration from daily operational tasks, such as creating users and topics, improves security and simplifies management. This separation is implemented by establishing dedicated namespaces, like `kafka-users` for user management and `kafka-topics` for topic management. By assigning appropriate Role-Based Access Control (RBAC) permissions, users can manage these resources without requiring access to the core Kafka infrastructure. The Kafka CRD's watchedNamespace property, within the entityOperator section, restricts the Topic and User Operators to their designated namespaces.

For instance:

[source,yaml]
----
  entityOperator:
    topicOperator:
      watchedNamespace: kafka-users
    userOperator:
      watchedNamespace: kafka-topics
----

Following this configuration, create Topic and User Custom Resource Definitions (CRDs) within the corresponding `kafka-topics` and `kafka-users` namespaces, respectively.